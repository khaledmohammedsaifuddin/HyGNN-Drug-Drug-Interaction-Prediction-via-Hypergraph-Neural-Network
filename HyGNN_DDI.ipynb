{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17105,
     "status": "ok",
     "timestamp": 1668551724935,
     "user": {
      "displayName": "kms hamim",
      "userId": "17071813694682210730"
     },
     "user_tz": 300
    },
    "id": "MIvYbPoP73FF",
    "outputId": "7053a63a-2828-434b-af24-3ac511a6f12e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 11232,
     "status": "ok",
     "timestamp": 1668551736164,
     "user": {
      "displayName": "kms hamim",
      "userId": "17071813694682210730"
     },
     "user_tz": 300
    },
    "id": "oG9atGpX4RvG"
   },
   "outputs": [],
   "source": [
    "!python '/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/EDA.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96b738wU8ME_"
   },
   "outputs": [],
   "source": [
    "!pip install karateclub\n",
    "!pip install node2vec\n",
    "!pip install dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8137,
     "status": "ok",
     "timestamp": 1668551767820,
     "user": {
      "displayName": "kms hamim",
      "userId": "17071813694682210730"
     },
     "user_tz": 300
    },
    "id": "Qi47bKUP8PmO",
    "outputId": "b8e41959-2e02-4c0c-8868-d8f4297fe33f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numba\n",
    "from numba import jit,njit, cuda\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score,precision_score, accuracy_score,average_precision_score,precision_recall_curve,auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from karateclub import DeepWalk\n",
    "from node2vec import Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1668552265152,
     "user": {
      "displayName": "kms hamim",
      "userId": "17071813694682210730"
     },
     "user_tz": 300
    },
    "id": "AzqjDr3e_UPN"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "converting to sparse matrix\n",
    "'''\n",
    "df=LEN=645 # or 645 if you use TWOSIDES dataset\n",
    "from scipy.sparse import coo_matrix\n",
    "nl=coo_matrix((df, df))\n",
    "nl.setdiag(1)\n",
    "#nl.toarray()\n",
    "values = nl.data\n",
    "indices = np.vstack((nl.row, nl.col))\n",
    "i = torch.LongTensor(indices)\n",
    "v = torch.FloatTensor(values)\n",
    "shape = nl.shape\n",
    "nl=torch.sparse_coo_tensor(i, v, torch.Size(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1668552266937,
     "user": {
      "displayName": "kms hamim",
      "userId": "17071813694682210730"
     },
     "user_tz": 300
    },
    "id": "ydFHKdf_L_MW"
   },
   "outputs": [],
   "source": [
    "chemicalsub_drug = torch.load('/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/hyG_drug_645_kmer_3.pt')#reading the hypergraph for drugbank dataset for 9-mer. change it based on your choice\n",
    "data_dict = {\n",
    "        ('node', 'in', 'edge'): (chemicalsub_drug[:,0], chemicalsub_drug[:,1]),        \n",
    "        ('edge', 'con', 'node'): (chemicalsub_drug[:,1], chemicalsub_drug[:,0])\n",
    "    }\n",
    "\n",
    "hyG = dgl.heterograph(data_dict)\n",
    "n_chemicalsub=822 #change the num of rows based on the dataset. row info. is available on data folder\n",
    "rows=n_chemicalsub\n",
    "n_hedge=LEN\n",
    "columns=n_hedge\n",
    "\n",
    "drug_X=nl\n",
    "hyG.ndata['h'] = {'edge' : torch.tensor(drug_X).type('torch.FloatTensor'), 'node' : torch.ones(rows, 128)}\n",
    "e_feat = torch.tensor(drug_X).type('torch.FloatTensor')\n",
    "v_feat = torch.ones(rows, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1668552269499,
     "user": {
      "displayName": "kms hamim",
      "userId": "17071813694682210730"
     },
     "user_tz": 300
    },
    "id": "cModgoYc3WxK",
    "outputId": "d1fe3e98-c229-4190-9d84-ef204ab79210"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'edge': 645, 'node': 822},\n",
       "      num_edges={('edge', 'con', 'node'): 31631, ('node', 'in', 'edge'): 31631},\n",
       "      metagraph=[('edge', 'node', 'con'), ('node', 'edge', 'in')])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1668552864281,
     "user": {
      "displayName": "kms hamim",
      "userId": "17071813694682210730"
     },
     "user_tz": 300
    },
    "id": "vR3bvhsYjS6y"
   },
   "outputs": [],
   "source": [
    "#3:\n",
    "src=[]\n",
    "dst=[]\n",
    "\n",
    "with open(\"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/edge_list_regular_graph_645.txt\") as fp:\n",
    "        for i,line in enumerate (fp):\n",
    "            info = line.strip().split()\n",
    "            src.append(info[0])\n",
    "            dst.append(info[1])\n",
    "\n",
    "src=np.asarray(src, dtype=np.int64) #to use in the data prerpocessing\n",
    "dst=np.asarray(dst, dtype=np.int64) #to use in the data prerpocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1668552866716,
     "user": {
      "displayName": "kms hamim",
      "userId": "17071813694682210730"
     },
     "user_tz": 300
    },
    "id": "KYhYK_AT82v0",
    "outputId": "2d92d584-4ecf-4f21-ddec-0e1c6d35a82e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=645, num_edges=126946,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Graph creation \n",
    "\"\"\"\n",
    "from dgl.data import DGLDataset\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class KarateClubDataset(DGLDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='karate_club')\n",
    "    def process(self):\n",
    "        edges_src = torch.from_numpy(src)\n",
    "        edges_dst = torch.from_numpy(dst)\n",
    "        self.graph = dgl.graph((edges_src, edges_dst), num_nodes=LEN)\n",
    "    def __getitem__(self, i):\n",
    "        return self.graph\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "dataset = KarateClubDataset()\n",
    "g = dataset[0]\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1368946,
     "status": "ok",
     "timestamp": 1657204850009,
     "user": {
      "displayName": "kms hamim",
      "userId": "17071813694682210730"
     },
     "user_tz": 300
    },
    "id": "TOecEkgG_Dvq",
    "outputId": "e55aff3a-a040-47c7-a1bb-dcf7df8d9b7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of edges:  2525926\n",
      "num of train POSITIVE edges:  306244 , num of test POSITIVE edges:  38280 num of val POSITIVE edges:  38280\n",
      "num of train NEGATIVE edges:  306244 , num of test NEGATIVE edges:  38280 num of val NEGATIVE edges:  38280\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Graph creation\n",
    "*************another approach ***********************************\n",
    "'''\n",
    "np.random.seed(42)\n",
    "# Split edge set for training and testing\n",
    "u, v = g.edges()\n",
    "eids = np.arange(g.number_of_edges())\n",
    "eids = np.random.permutation(eids)\n",
    "eids=eids.tolist()\n",
    "test_size = int(len(eids) * 0.1)\n",
    "val_size = int(len(eids) * 0.1)\n",
    "train_size=len(eids)-(test_size+val_size)\n",
    "test_size1=int(test_size/2)\n",
    "\n",
    "\n",
    "T1=[]\n",
    "T2=[]\n",
    "numba.njit(target=\"cuda\")\n",
    "def func2(T1):\n",
    "  for i in eids:\n",
    "    m, n = int(u[i]), int(v[i])\n",
    "    c=(m,n)\n",
    "    T1.append(c)\n",
    "  return T1  \n",
    "function1=jit(parallel=True) (func2)\n",
    "T1 = function1(T1)\n",
    "\n",
    "\n",
    "numba.njit(target=\"cuda\")\n",
    "def func3(T1,T2):\n",
    "  for i in T1:\n",
    "    m=i[0]\n",
    "    n=i[1]\n",
    "    c=(m,n)\n",
    "    d=(n,m)\n",
    "    if c not in T2 and d not in T2:\n",
    "      T2.append(c)\n",
    "    if len(T2)>=test_size1:\n",
    "      break  \n",
    "  return T2\n",
    "function1=jit(parallel=True) (func3)\n",
    "T2 = function1(T1,T2)\n",
    "\n",
    "\n",
    "test_eids=[]\n",
    "numba.njit(target=\"cuda\")\n",
    "def func4(T2,test_eids,g):\n",
    "  for i in T2:\n",
    "    m= i[0]\n",
    "    n=i[1]\n",
    "    c=g.edge_id(m,n)\n",
    "    d=g.edge_id(n,m)\n",
    "    test_eids.append(c)\n",
    "    test_eids.append(d)\n",
    "  return test_eids  \n",
    "function1=jit(parallel=True) (func4)\n",
    "test_eids = function1(T2,test_eids,g)\n",
    "\n",
    "val_eids=[]\n",
    "numba.njit(target=\"cuda\")\n",
    "def func5(eids,test_eids,val_eids):\n",
    "  i=0\n",
    "  for j in eids:\n",
    "    a=int(eids[i])\n",
    "    if a not in test_eids and len(val_eids)<val_size:\n",
    "      val_eids.append(a)  \n",
    "    i=i+1\n",
    "  return val_eids    \n",
    "function1=jit(parallel=True) (func5)\n",
    "val_eids = function1(eids,test_eids,val_eids)     \n",
    "\n",
    "train_eids=[]\n",
    "numba.njit(target=\"cuda\")\n",
    "def func6(eids,test_eids,val_eids,train_eids):\n",
    "  i=0\n",
    "  for j in eids:\n",
    "    a=int(eids[i])\n",
    "    if a not in test_eids and a not in val_eids and len(train_eids)<train_size:\n",
    "      train_eids.append(a)  \n",
    "    i=i+1\n",
    "  return train_eids    \n",
    "function1=jit(parallel=True) (func6)\n",
    "train_eids = function1(eids,test_eids,val_eids,train_eids)     \n",
    "\n",
    "# main one. splitting into train and test set.\n",
    "test_pos_u, test_pos_v = u[test_eids], v[test_eids]\n",
    "train_pos_u, train_pos_v = u[train_eids], v[train_eids]\n",
    "val_pos_u, val_pos_v = u[val_eids], v[val_eids]\n",
    "\n",
    "\n",
    "# Find all negative edges and split them for training and testing\n",
    "adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n",
    "adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n",
    "neg_u, neg_v = np.where(adj_neg != 0)\n",
    "\n",
    "neg_eids = np.random.choice(len(neg_u), g.number_of_edges(),replace=False)\n",
    "g_=nx.from_numpy_matrix(adj_neg)\n",
    "g_=dgl.from_networkx(g_)\n",
    "\n",
    "T1=[]\n",
    "T2=[]\n",
    "numba.njit(target=\"cuda\")\n",
    "def func2(T1):\n",
    "  for i in neg_eids:\n",
    "    m, n = int(neg_u[i]), int(neg_v[i])\n",
    "    c=(m,n)\n",
    "    T1.append(c)\n",
    "  return T1  \n",
    "function1=jit(parallel=True) (func2)\n",
    "T1 = function1(T1)\n",
    "\n",
    "\n",
    "numba.njit(target=\"cuda\")\n",
    "def func3(T1,T2):\n",
    "  for i in T1:\n",
    "    m=i[0]\n",
    "    n=i[1]\n",
    "    c=(m,n)\n",
    "    d=(n,m)\n",
    "    if c not in T2 and d not in T2:\n",
    "      T2.append(c)\n",
    "    if len(T2)>=test_size1:\n",
    "      break  \n",
    "  return T2\n",
    "function1=jit(parallel=True) (func3)\n",
    "T2 = function1(T1,T2)\n",
    "\n",
    "\n",
    "test_neg_eids=[]\n",
    "numba.njit(target=\"cuda\")\n",
    "def func4(T2,test_eids,g):\n",
    "  for i in T2:\n",
    "    m= i[0]\n",
    "    n=i[1]\n",
    "    c=g.edge_id(m,n)\n",
    "    d=g.edge_id(n,m)\n",
    "    test_neg_eids.append(c)\n",
    "    test_neg_eids.append(d)\n",
    "  return test_neg_eids  \n",
    "function1=jit(parallel=True) (func4)\n",
    "test_neg_eids = function1(T2,test_neg_eids,g_)\n",
    "\n",
    "val_neg_eids=[]\n",
    "numba.njit(target=\"cuda\")\n",
    "def func5(neg_eids,test_neg_eids,val_neg_eids):\n",
    "  i=0\n",
    "  for j in neg_eids:\n",
    "    a=int(neg_eids[i])\n",
    "    if a not in test_neg_eids and len(val_neg_eids)<(val_size):\n",
    "      val_neg_eids.append(a)  \n",
    "    i=i+1\n",
    "  return val_neg_eids    \n",
    "function1=jit(parallel=True) (func5)\n",
    "val_neg_eids = function1(neg_eids,test_neg_eids,val_neg_eids)  \n",
    "\n",
    "train_neg_eids=[]\n",
    "numba.njit(target=\"cuda\")\n",
    "def func6(neg_eids,test_neg_eids,val_neg_eids,train_neg_eids):\n",
    "  i=0\n",
    "  for j in neg_eids:\n",
    "    a=int(neg_eids[i])\n",
    "    if a not in test_neg_eids and a not in val_neg_eids and len(train_neg_eids)<(train_size):\n",
    "      train_neg_eids.append(a)  \n",
    "    i=i+1\n",
    "  return train_neg_eids    \n",
    "function1=jit(parallel=True) (func6)\n",
    "train_neg_eids = function1(neg_eids,test_neg_eids,val_neg_eids,train_neg_eids)     \n",
    "\n",
    "test_neg_u, test_neg_v = neg_u[test_neg_eids], neg_v[test_neg_eids]\n",
    "train_neg_u, train_neg_v = neg_u[train_neg_eids], neg_v[train_neg_eids]\n",
    "val_neg_u, val_neg_v = neg_u[val_neg_eids], neg_v[val_neg_eids]\n",
    "\n",
    "train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())# this is just the edge_list\n",
    "train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n",
    "test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n",
    "test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())\n",
    "val_pos_g = dgl.graph((val_pos_u, val_pos_v), num_nodes=g.number_of_nodes())\n",
    "val_neg_g = dgl.graph((val_neg_u, val_neg_v), num_nodes=g.number_of_nodes())\n",
    "\n",
    "print(\"Total number of edges: \",g_.number_of_edges())\n",
    "print(\"num of train POSITIVE edges: \",train_pos_g.number_of_edges(),\", num of test POSITIVE edges: \",test_pos_g.number_of_edges(),\"num of val POSITIVE edges: \",val_pos_g.number_of_edges())\n",
    "print(\"num of train NEGATIVE edges: \",train_neg_g.number_of_edges(),\", num of test NEGATIVE edges: \",test_neg_g.number_of_edges(),\"num of val NEGATIVE edges: \",val_neg_g.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WoqvTtA0E45"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Data prep for CASTER. This part returns negative edges\n",
    "'''\n",
    "\n",
    "neg_u, neg_v\n",
    "neg_u_s=[]\n",
    "neg_v_s=[]\n",
    "\n",
    "numba.njit(target=\"cuda\")\n",
    "def func(neg_u):\n",
    "  for i in neg_u:\n",
    "    for j,k in D.items():\n",
    "      if i==k:\n",
    "        neg_u_s.append(j)\n",
    "        print('o')\n",
    "        break\n",
    "  return neg_u_s      \n",
    "function1=jit(parallel=True) (func)\n",
    "neg_u_s = function1(neg_u) \n",
    "\n",
    "numba.njit(target=\"cuda\")\n",
    "def func(neg_v):\n",
    "  for i in neg_v:\n",
    "    for j,k in D.items():\n",
    "      if i==k:\n",
    "        neg_v_s.append(j)\n",
    "        print('o')\n",
    "        break\n",
    "  return neg_v_s      \n",
    "function1=jit(parallel=True) (func)\n",
    "neg_v_s = function1(neg_v)   \n",
    "\n",
    "D_1={}\n",
    "for i in range (len(df[1])):\n",
    "    D_1[df[0][i]]=df[1][i]\n",
    "\n",
    "neg_u_s,neg_v_s\n",
    "neg_u_s1=[]\n",
    "neg_v_s1=[]\n",
    "\n",
    "numba.njit(target=\"cuda\")\n",
    "def func(neg_u_s):\n",
    "  for i in neg_u_s:\n",
    "    for j,k in D_1.items():\n",
    "      if i==j:\n",
    "        neg_u_s1.append(k)\n",
    "        print('o')\n",
    "        break\n",
    "  return neg_u_s1     \n",
    "function1=jit(parallel=True) (func)\n",
    "neg_u_s1 = function1(neg_u_s) \n",
    "\n",
    "numba.njit(target=\"cuda\")\n",
    "def func(neg_v_s):\n",
    "  for i in neg_v_s:\n",
    "    for j,k in D_1.items():\n",
    "      if i==j:\n",
    "        neg_v_s1.append(k)\n",
    "        print('o')\n",
    "        break\n",
    "  return neg_v_s1      \n",
    "function1=jit(parallel=True) (func)\n",
    "neg_v_s1 = function1(neg_v_s)  \n",
    "\n",
    "LEN=len(u)\n",
    "a_list_neg=[0]*LEN\n",
    "len(a_list_neg)\n",
    "\n",
    "df_re_reg = pd.DataFrame(list(zip(neg_u_s, neg_u_s1,neg_v_s,neg_v_s1,a_list_neg)),columns =['Drug1_ID', 'Drug1_SMILES','Drug2_ID','Drug2_SMILES','label'])\n",
    "df_re_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ls--9GqK31S0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Data prep for CASTER. This part returns positive edges\n",
    "'''\n",
    "u,v\n",
    "u_s=[]\n",
    "v_s=[]\n",
    "\n",
    "numba.njit(target=\"cuda\")\n",
    "def func(u):\n",
    "  for i in u:\n",
    "    for j,k in D.items():\n",
    "      if i==k:\n",
    "        u_s.append(j)\n",
    "        print('o')\n",
    "        break\n",
    "  return u_s      \n",
    "function1=jit(parallel=True) (func)\n",
    "u_s = function1(u) \n",
    "\n",
    "numba.njit(target=\"cuda\")\n",
    "def func(v):\n",
    "  for i in v:\n",
    "    for j,k in D.items():\n",
    "      if i==k:\n",
    "        v_s.append(j)\n",
    "        print('o')\n",
    "        break\n",
    "  return v_s      \n",
    "function1=jit(parallel=True) (func)\n",
    "v_s = function1(v)   \n",
    "\n",
    "D_1={}\n",
    "for i in range (len(df[1])):\n",
    "    D_1[df[0][i]]=df[1][i]\n",
    "\n",
    "u_s,v_s\n",
    "u_s1=[]\n",
    "v_s1=[]\n",
    "\n",
    "numba.njit(target=\"cuda\")\n",
    "def func(u_s):\n",
    "  for i in u_s:\n",
    "    for j,k in D_1.items():\n",
    "      if i==j:\n",
    "        u_s1.append(k)\n",
    "        print('o')\n",
    "        break\n",
    "  return u_s1     \n",
    "function1=jit(parallel=True) (func)\n",
    "u_s1 = function1(u_s) \n",
    "\n",
    "numba.njit(target=\"cuda\")\n",
    "def func(v_s):\n",
    "  for i in v_s:\n",
    "    for j,k in D_1.items():\n",
    "      if i==j:\n",
    "        v_s1.append(k)\n",
    "        print('o')\n",
    "        break\n",
    "  return v_s1      \n",
    "function1=jit(parallel=True) (func)\n",
    "v_s1 = function1(v_s)  \n",
    "\n",
    "LEN=len(u)\n",
    "a_list_pos=[1]*LEN\n",
    "len(a_list_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLUkDmIl9Yo1"
   },
   "outputs": [],
   "source": [
    "df_re = pd.DataFrame(list(zip(u_s, u_s1,v_s,v_s1,a_list_pos)),columns =['Drug1_ID', 'Drug1_SMILES','Drug2_ID','Drug2_SMILES','label'])\n",
    "print(len(df_re_reg))\n",
    "df_re = df_re.append(df_re_reg, ignore_index=True)\n",
    "df_re = df_re.sample(frac=1).reset_index(drop=True)\n",
    "df_re.to_csv('/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/TDC_DrugBank_FOR_CASTER.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "executionInfo": {
     "elapsed": 141,
     "status": "error",
     "timestamp": 1668552895009,
     "user": {
      "displayName": "kms hamim",
      "userId": "17071813694682210730"
     },
     "user_tz": 300
    },
    "id": "7NQ16ZlGNH47",
    "outputId": "41349e07-0779-4dd8-a617-89d026d249e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of edges:  126946\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-867f7d857d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total number of edges: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"num of train POSITIVE edges: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_pos_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\", num of test POSITIVE edges: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_pos_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"num of val POSITIVE edges: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_pos_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"num of train NEGATIVE edges: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_neg_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\", num of test NEGATIVE edges: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_neg_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"num of val NEGATIVE edges: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_neg_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mg_for_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_eids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_pos_g' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Total number of edges: \",g.number_of_edges())\n",
    "print(\"num of train POSITIVE edges: \",train_pos_g.number_of_edges(),\", num of test POSITIVE edges: \",test_pos_g.number_of_edges(),\"num of val POSITIVE edges: \",val_pos_g.number_of_edges())\n",
    "print(\"num of train NEGATIVE edges: \",train_neg_g.number_of_edges(),\", num of test NEGATIVE edges: \",test_neg_g.number_of_edges(),\"num of val NEGATIVE edges: \",val_neg_g.number_of_edges())\n",
    "\n",
    "g_for_baseline = dgl.remove_edges(g, test_eids)\n",
    "#dgl.save_graphs(\"drive/My Drive/Colab Notebooks/Hypergraph_my/a.bin\",g_for_baseline)\n",
    "g_for_baseline = dgl.add_self_loop(g_for_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_U_zGixKUbG"
   },
   "outputs": [],
   "source": [
    "def compute_loss(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "def compute_auc(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
    "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy() \n",
    "    precision, recall, thresholds = precision_recall_curve(labels, scores)\n",
    "    auc_precision_recall = auc(recall, precision)\n",
    "    return roc_auc_score(labels, scores),auc(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBFa3jJD9Jo2"
   },
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "class DotPredictor(nn.Module):\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h # here h is 822 drug features and g is the pos/neg train/test graph which is nothing but edge lis\n",
    "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            return g.edata['score'][:, 0]\n",
    "\n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, h_feats):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
    "        self.W2 = nn.Linear(h_feats, 1)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        h = torch.cat([edges.src['h'], edges.dst['h']], 1)\n",
    "        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            g.apply_edges(self.apply_edges)\n",
    "            return g.edata['score']\n",
    "\n",
    "\n",
    "decoder = MLPPredictor(128)\n",
    "#decoder = DotPredictor()# You can replace DotPredictor with MLPPredictor.\n",
    "#opt = torch.optim.Adam(list(model.parameters()) + list(pred.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvzS_z3OGNdd"
   },
   "outputs": [],
   "source": [
    "class HyGNN(nn.Module):\n",
    "    # edge attention  version\n",
    "    def __init__(self, input_dim, query_dim, vertex_dim, edge_dim, dropout):\n",
    "        super(HyGNN, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.query_dim = query_dim\n",
    "        self.vtx_lin_1layer = torch.nn.Linear(input_dim, vertex_dim)\n",
    "        self.vtx_lin = torch.nn.Linear(vertex_dim, vertex_dim)\n",
    "        \n",
    "        self.qe_lin = torch.nn.Linear(edge_dim, query_dim)\n",
    "        self.kv_lin = torch.nn.Linear(vertex_dim, query_dim)\n",
    "        self.vv_lin = torch.nn.Linear(vertex_dim, edge_dim)\n",
    "        \n",
    "        self.qv_lin = torch.nn.Linear(vertex_dim, query_dim)\n",
    "        self.ke_lin = torch.nn.Linear(edge_dim, query_dim)\n",
    "        self.ve_lin = torch.nn.Linear(edge_dim, vertex_dim)\n",
    "        \n",
    "      \n",
    "    def attention(self, edges):\n",
    "        attn_score = F.leaky_relu((edges.src['k'] * edges.dst['q']).sum(-1))\n",
    "        return {'Attn': attn_score/np.sqrt(self.query_dim)}\n",
    "    \n",
    "    def message_func(self, edges):\n",
    "        return {'v': edges.src['v'], 'Attn': edges.data['Attn']}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        attention_score = F.softmax((nodes.mailbox['Attn']), dim=1)\n",
    "        aggr = torch.sum(attention_score.unsqueeze(-1) * nodes.mailbox['v'], dim=1)\n",
    "        return {'h': aggr}\n",
    "\n",
    "    def forward(self, hyG, vfeat, efeat, first_layer, last_layer):\n",
    "            if first_layer:\n",
    "                feat_e = self.vtx_lin_1layer(efeat)\n",
    "            else:\n",
    "                feat_e = self.vtx_lin(efeat)        \n",
    "            feat_v = vfeat\n",
    "\n",
    "            hyG.ndata['h'] = {'edge': feat_e}\n",
    "            hyG.ndata['k'] = {'edge' : self.ke_lin(feat_e)}\n",
    "            hyG.ndata['v'] = {'edge' : self.ve_lin(feat_e)}\n",
    "            hyG.ndata['q'] = {'node' : self.qv_lin(feat_v)}\n",
    "            hyG.apply_edges(self.attention, etype='con')\n",
    "            hyG.update_all(self.message_func, self.reduce_func, etype='con')\n",
    "            feat_v = hyG.ndata['h']['node']\n",
    "            hyG.ndata['k'] = {'node' : self.kv_lin(feat_v)}\n",
    "            hyG.ndata['v'] = {'node' : self.vv_lin(feat_v)}\n",
    "            hyG.ndata['q'] = {'edge' : self.qe_lin(feat_e)}\n",
    "            hyG.apply_edges(self.attention, etype='in')\n",
    "            hyG.update_all(self.message_func, self.reduce_func, etype='in')\n",
    "            feat_e = hyG.ndata['h']['edge']\n",
    "            \n",
    "            if not last_layer :\n",
    "                feat_v = F.dropout(feat_v, self.dropout)\n",
    "            if last_layer:\n",
    "              \n",
    "                return feat_v, feat_e\n",
    "            else:\n",
    "                return [hyG, feat_v, feat_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17tBcT8M-BGG"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.gat1 = HyGNN(drug_X.shape[1],64,128,128,0.5)\n",
    " \n",
    "    def forward(self,hyG, v_feat, e_feat,f,l):   \n",
    "        h = self.gat1(hyG, v_feat, e_feat,f,l)\n",
    "        return h\n",
    "model = Model()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jLSMx5T-ItX"
   },
   "outputs": [],
   "source": [
    "# ----------- 3. set up loss and optimizer -------------- #\n",
    "optimizer = torch.optim.Adam(itertools.chain(model.parameters(), decoder.parameters()), lr=0.005)\n",
    "best_val_loss=1e10\n",
    "patience=0\n",
    "\n",
    "for e in range(500):\n",
    "    # forward\n",
    "    model.train()\n",
    "    h=model(hyG, v_feat, e_feat,True,True)\n",
    "    h=h[1]\n",
    "    pos_score = decoder(train_pos_g, h)\n",
    "    neg_score = decoder(train_neg_g, h)\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "    \n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      model.eval()\n",
    "      pos_score = decoder(val_pos_g, h)\n",
    "      neg_score = decoder(val_neg_g, h)\n",
    "      #val_acc=compute_auc(pos_score, neg_score)\n",
    "      val_loss = compute_loss(pos_score, neg_score)\n",
    "      if val_loss<best_val_loss:\n",
    "        best_val_loss=val_loss\n",
    "        H=h\n",
    "        E=e\n",
    "        patience=0\n",
    "        torch.save(decoder.state_dict(),'latest.pth')\n",
    "      else:\n",
    "        patience+=1  \n",
    "    if patience>200:\n",
    "      break\n",
    "\n",
    "    if e % 10 == 0:\n",
    "      print('In epoch {}, train loss: {:.4f}, val loss: {:.4f} (best val loss: {:.4f})'.format(e, loss, val_loss, best_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcRFtgZoPWSb"
   },
   "outputs": [],
   "source": [
    "decoder.load_state_dict(torch.load('latest.pth'))\n",
    "with torch.no_grad():\n",
    "  model.eval()\n",
    "  \n",
    "  pos_score = decoder(test_pos_g, H)\n",
    "  neg_score = decoder(test_neg_g, H)\n",
    "  test_acc=compute_auc(pos_score, neg_score)\n",
    "\n",
    "scores = torch.cat([pos_score, neg_score])\n",
    "labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
    "\n",
    "m1 = tf.keras.metrics.BinaryAccuracy()\n",
    "m1.update_state(labels,scores)\n",
    "\n",
    "sig_scores=F.sigmoid(scores)\n",
    "m2 = tf.keras.metrics.Precision()\n",
    "m2.update_state(labels,sig_scores)\n",
    "M2=m2.result().numpy()\n",
    "\n",
    "m3 = tf.keras.metrics.Recall()\n",
    "m3.update_state(labels,sig_scores)\n",
    "M3=m3.result().numpy()\n",
    "\n",
    "F1=2*(M2*M3)/(M2+M3)\n",
    "print(' Best Epoch: {}, Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-score {:.4f}, ROC-AUC {:.4f}, PR-AUC {:.4f}'.format( E,m1.result().numpy(), M2, M3, F1,test_acc[0],test_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oad6nWOtTaUj"
   },
   "outputs": [],
   "source": [
    "################################################## Case study #########################################3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6anF0igYM2kW"
   },
   "outputs": [],
   "source": [
    "lst=[]\n",
    "for i in range (25388,26410): # range s ur choice \n",
    "  if sig_scores[i]<=0.0000001: # use a large threshold say 0.9\n",
    "    #print(sig_scores[i])\n",
    "    lst.append(i)\n",
    "print(len(lst))\n",
    "sig_scores[lst[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMzBtsy3cThR"
   },
   "outputs": [],
   "source": [
    "c=0\n",
    "for i in lst:\n",
    "  j=i-25388\n",
    "  print(sig_scores[i],test_neg_u[j],test_neg_v[j])\n",
    "  for m,n in D.items():\n",
    "    if n==test_neg_v[j]:\n",
    "      #print(m)\n",
    "      break\n",
    "  c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 145,
     "status": "ok",
     "timestamp": 1656814293257,
     "user": {
      "displayName": "kms hamim",
      "userId": "17071813694682210730"
     },
     "user_tz": 300
    },
    "id": "CxEEEHk5_OaI",
    "outputId": "61951f98-516c-4c60-9d10-1a69ae161dea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493\n"
     ]
    }
   ],
   "source": [
    "for i,j in D.items():\n",
    "  if i=='CID000003198':\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 155,
     "status": "ok",
     "timestamp": 1656812142922,
     "user": {
      "displayName": "kms hamim",
      "userId": "17071813694682210730"
     },
     "user_tz": 300
    },
    "id": "UEFLCweiN0n1",
    "outputId": "9d7b2cbb-52aa-40df-bb36-2903d807518f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_neg_g.has_edge_between(363,290)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 176,
     "status": "ok",
     "timestamp": 1656814304629,
     "user": {
      "displayName": "kms hamim",
      "userId": "17071813694682210730"
     },
     "user_tz": 300
    },
    "id": "uT4JhHONQYmb",
    "outputId": "189599d6-fc03-4570-dff7-9679de3db8c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# where we are cross-checking between what we predicted and what is in the DrugBank (label)\n",
    "for i in range (len(mat)):\n",
    "  for j in range (len(mat)):\n",
    "    if i==493  and j==639:\n",
    "      print(mat[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LV6KBq83O003"
   },
   "outputs": [],
   "source": [
    "for i in range(len(test_neg_u)):\n",
    "  if test_neg_u[i]==126 and test_neg_v[i]==338:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltOucGU3U8fd"
   },
   "outputs": [],
   "source": [
    "for i in range (len(test_pos_u)):\n",
    "  if test_pos_u[i]==656 and test_pos_v[i]==622:\n",
    "    print(sig_scores[i])\n",
    "  if test_pos_u[i]==622 and test_pos_v[i]==656:\n",
    "    print(sig_scores[i])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3wFelXBH5rQ"
   },
   "outputs": [],
   "source": [
    "********************************************************** GCN conv with ML **********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4349316,
     "status": "ok",
     "timestamp": 1657258170623,
     "user": {
      "displayName": "kms hamim",
      "userId": "17071813694682210730"
     },
     "user_tz": 300
    },
    "id": "7MFMLGBXM1F5",
    "outputId": "63b80bd6-674e-4965-d3f4-0e06d906adf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of edges:  382804\n",
      "num of train POSITIVE edges:  76561 , num of test POSITIVE edges:  306242\n",
      "num of train NEGATIVE edges:  76561 , num of test NEGATIVE edges:  306242\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "*************another approach\n",
    "Train, test (both positive and negative) graphs creation for training and testing purpose from the dgl graph (g)\n",
    "Source: DGL\n",
    "'''\n",
    "np.random.seed(42)\n",
    "# Split edge set for training and testing\n",
    "u, v = g.edges()\n",
    "eids = np.arange(g.number_of_edges())\n",
    "eids = np.random.permutation(eids)\n",
    "eids=eids.tolist()\n",
    "test_size = int(len(eids) * 0.8)\n",
    "train_size=len(eids)-(test_size)\n",
    "test_size1=int(test_size/2)\n",
    "\n",
    "\n",
    "T1=[]\n",
    "T2=[]\n",
    "numba.njit(target=\"cuda\")\n",
    "def func2(T1):\n",
    "  for i in eids:\n",
    "    m, n = int(u[i]), int(v[i])\n",
    "    c=(m,n)\n",
    "    T1.append(c)\n",
    "  return T1  \n",
    "function1=jit(parallel=True) (func2)\n",
    "T1 = function1(T1)\n",
    "\n",
    "\n",
    "numba.njit(target=\"cuda\")\n",
    "def func3(T1,T2):\n",
    "  for i in T1:\n",
    "    m=i[0]\n",
    "    n=i[1]\n",
    "    c=(m,n)\n",
    "    d=(n,m)\n",
    "    if c not in T2 and d not in T2:\n",
    "      T2.append(c)\n",
    "    if len(T2)>=test_size1:\n",
    "      break  \n",
    "  return T2\n",
    "function1=jit(parallel=True) (func3)\n",
    "T2 = function1(T1,T2)\n",
    "\n",
    "\n",
    "test_eids=[]\n",
    "numba.njit(target=\"cuda\")\n",
    "def func4(T2,test_eids,g):\n",
    "  for i in T2:\n",
    "    m= i[0]\n",
    "    n=i[1]\n",
    "    c=g.edge_id(m,n)\n",
    "    d=g.edge_id(n,m)\n",
    "    test_eids.append(c)\n",
    "    test_eids.append(d)\n",
    "  return test_eids  \n",
    "function1=jit(parallel=True) (func4)\n",
    "test_eids = function1(T2,test_eids,g)\n",
    "\n",
    "\n",
    "train_eids=[]\n",
    "numba.njit(target=\"cuda\")\n",
    "def func6(eids,test_eids,train_eids):\n",
    "  i=0\n",
    "  for j in eids:\n",
    "    a=int(eids[i])\n",
    "    if a not in test_eids  and len(train_eids)<train_size:\n",
    "      train_eids.append(a)  \n",
    "    i=i+1\n",
    "  return train_eids    \n",
    "function1=jit(parallel=True) (func6)\n",
    "train_eids = function1(eids,test_eids,train_eids)     \n",
    "\n",
    "# main one. splitting into train and test set.\n",
    "test_pos_u, test_pos_v = u[test_eids], v[test_eids]\n",
    "train_pos_u, train_pos_v = u[train_eids], v[train_eids]\n",
    "\n",
    "\n",
    "\n",
    "# Find all negative edges and split them for training and testing\n",
    "adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n",
    "adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n",
    "neg_u, neg_v = np.where(adj_neg != 0)\n",
    "\n",
    "neg_eids = np.random.choice(len(neg_u), g.number_of_edges(),replace=False)\n",
    "g_=nx.from_numpy_matrix(adj_neg)\n",
    "g_=dgl.from_networkx(g_)\n",
    "\n",
    "T1=[]\n",
    "T2=[]\n",
    "numba.njit(target=\"cuda\")\n",
    "def func2(T1):\n",
    "  for i in neg_eids:\n",
    "    m, n = int(neg_u[i]), int(neg_v[i])\n",
    "    c=(m,n)\n",
    "    T1.append(c)\n",
    "  return T1  \n",
    "function1=jit(parallel=True) (func2)\n",
    "T1 = function1(T1)\n",
    "\n",
    "\n",
    "numba.njit(target=\"cuda\")\n",
    "def func3(T1,T2):\n",
    "  for i in T1:\n",
    "    m=i[0]\n",
    "    n=i[1]\n",
    "    c=(m,n)\n",
    "    d=(n,m)\n",
    "    if c not in T2 and d not in T2:\n",
    "      T2.append(c)\n",
    "    if len(T2)>=test_size1:\n",
    "      break  \n",
    "  return T2\n",
    "function1=jit(parallel=True) (func3)\n",
    "T2 = function1(T1,T2)\n",
    "\n",
    "\n",
    "test_neg_eids=[]\n",
    "numba.njit(target=\"cuda\")\n",
    "def func4(T2,test_eids,g):\n",
    "  for i in T2:\n",
    "    m= i[0]\n",
    "    n=i[1]\n",
    "    c=g.edge_id(m,n)\n",
    "    d=g.edge_id(n,m)\n",
    "    test_neg_eids.append(c)\n",
    "    test_neg_eids.append(d)\n",
    "  return test_neg_eids  \n",
    "function1=jit(parallel=True) (func4)\n",
    "test_neg_eids = function1(T2,test_neg_eids,g_)\n",
    "\n",
    "\n",
    "train_neg_eids=[]\n",
    "numba.njit(target=\"cuda\")\n",
    "def func6(neg_eids,test_neg_eids,train_neg_eids):\n",
    "  i=0\n",
    "  for j in neg_eids:\n",
    "    a=int(neg_eids[i])\n",
    "    if a not in test_neg_eids and len(train_neg_eids)<(train_size):\n",
    "      train_neg_eids.append(a)  \n",
    "    i=i+1\n",
    "  return train_neg_eids    \n",
    "function1=jit(parallel=True) (func6)\n",
    "train_neg_eids = function1(neg_eids,test_neg_eids,train_neg_eids)     \n",
    "\n",
    "test_neg_u, test_neg_v = neg_u[test_neg_eids], neg_v[test_neg_eids]\n",
    "train_neg_u, train_neg_v = neg_u[train_neg_eids], neg_v[train_neg_eids]\n",
    "\n",
    "\n",
    "train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())# this is just the edge_list\n",
    "train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n",
    "test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n",
    "test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())\n",
    "\n",
    "print(\"Total number of edges: \",g.number_of_edges())\n",
    "print(\"num of train POSITIVE edges: \",train_pos_g.number_of_edges(),\", num of test POSITIVE edges: \",test_pos_g.number_of_edges())\n",
    "print(\"num of train NEGATIVE edges: \",train_neg_g.number_of_edges(),\", num of test NEGATIVE edges: \",test_neg_g.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PCIgtyzM2x1"
   },
   "outputs": [],
   "source": [
    "g_for_baseline = dgl.remove_edges(g, test_eids)\n",
    "#dgl.save_graphs(\"drive/My Drive/Colab Notebooks/Hypergraph_my/a.bin\",g_for_baseline)\n",
    "g_for_baseline = dgl.add_self_loop(g_for_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAkTpAjJzJ9W"
   },
   "outputs": [],
   "source": [
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, h_feats):\n",
    "        super().__init__()\n",
    "    def apply_edges(self, edges):\n",
    "        h = torch.cat([edges.src['h'], edges.dst['h']], 1).squeeze(1)\n",
    "        return {'score': h.squeeze(1)}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            g.apply_edges(self.apply_edges)\n",
    "            return g.edata['score']\n",
    "\n",
    "decoder = MLPPredictor(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjwnjV_2H-a_"
   },
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "# ----------- 2. create model -------------- #\n",
    "# build a two-layer GraphSAGE model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, h_feats)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "model = GCN(LEN, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykcPsc-i2d9z"
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 829,
     "status": "ok",
     "timestamp": 1657258326013,
     "user": {
      "displayName": "kms hamim",
      "userId": "17071813694682210730"
     },
     "user_tz": 300
    },
    "id": "DB8T5SvM3Zkb",
    "outputId": "47d79b66-2681-4180-eca6-5b4d65719691"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_feat=torch.zeros(LEN,LEN)\n",
    "e_feat.fill_diagonal_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ELIEN_q3LHo-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "when u want to use saved GCN embeddings\n",
    "'''\n",
    "h =np.loadtxt(\"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/afterTrainingEmbedsGAT.txt\")\n",
    "h=torch.tensor(h)\n",
    "h=h.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imlq53oXzqRE"
   },
   "outputs": [],
   "source": [
    "h = model(g_for_baseline, e_feat)\n",
    "pos_score_train = decoder(train_pos_g, h)\n",
    "neg_score_train = decoder(train_neg_g, h)\n",
    "pos_score_test = decoder(test_pos_g, h)\n",
    "neg_score_test = decoder(test_neg_g, h)\n",
    "\n",
    "scores_train = torch.cat([pos_score_train, neg_score_train]).detach().numpy()\n",
    "labels_train = torch.cat([torch.ones(pos_score_train.shape[0]), torch.zeros(neg_score_train.shape[0])]).detach().numpy()\n",
    "scores_test = torch.cat([pos_score_test, neg_score_test]).detach().numpy()\n",
    "labels_test = torch.cat([torch.ones(pos_score_test.shape[0]), torch.zeros(neg_score_test.shape[0])]).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LN00S7FpODnJ"
   },
   "outputs": [],
   "source": [
    "print(\"LR\")\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "lr.fit(scores_train, labels_train)\n",
    "predictions = lr.predict(scores_test)\n",
    "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n",
    "\n",
    "print(\"NB\")\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(scores_train, labels_train)\n",
    "predictions = gnb.predict(scores_test)\n",
    "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n",
    "\n",
    "print(\"RF\")\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(scores_train, labels_train)\n",
    "predictions = clf.predict(scores_test)\n",
    "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gGnogCKReMV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oivgp2CGWg7I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0o6fss5WhhG"
   },
   "outputs": [],
   "source": [
    "********************************************************* Graph SAGE with ML ********************************************8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pf96mSJ4WhQd"
   },
   "outputs": [],
   "source": [
    "from dgl.nn import SAGEConv\n",
    "\n",
    "# ----------- 2. create model -------------- #\n",
    "# build a two-layer GraphSAGE model\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, 'pool')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, 'pool')\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "model = GraphSAGE(1706, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4lNziClA8I3"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "when u want to use saved GCN embeddings\n",
    "'''\n",
    "h =np.loadtxt(\"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/afterTrainingEmbedsGS_DB.txt\")\n",
    "h=torch.tensor(h)\n",
    "h=h.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9wKMe8-XRur"
   },
   "outputs": [],
   "source": [
    "#h = model(g_for_baseline, e_feat)\n",
    "pos_score_train = decoder(train_pos_g, h)\n",
    "neg_score_train = decoder(train_neg_g, h)\n",
    "pos_score_test = decoder(test_pos_g, h)\n",
    "neg_score_test = decoder(test_neg_g, h)\n",
    "\n",
    "scores_train = torch.cat([pos_score_train, neg_score_train]).detach().numpy()\n",
    "labels_train = torch.cat([torch.ones(pos_score_train.shape[0]), torch.zeros(neg_score_train.shape[0])]).detach().numpy()\n",
    "scores_test = torch.cat([pos_score_test, neg_score_test]).detach().numpy()\n",
    "labels_test = torch.cat([torch.ones(pos_score_test.shape[0]), torch.zeros(neg_score_test.shape[0])]).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35551,
     "status": "ok",
     "timestamp": 1657258486790,
     "user": {
      "displayName": "kms hamim",
      "userId": "17071813694682210730"
     },
     "user_tz": 300
    },
    "id": "kNGAxn1vXU8b",
    "outputId": "7b883f34-0ca7-457b-adf3-10c087ff07b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "accuracy:  0.5771987513143201 precision:  0.577202227668201 recall:  0.5771987513143201 f1-score:  0.5771939916611587 ROC-AUC:  0.57719875131432 PR-AUC:  0.6834805709662735\n",
      "NB\n",
      "accuracy:  0.5452273038969181 precision:  0.5455359594068304 recall:  0.5452273038969181 f1-score:  0.54445535153448 ROC-AUC:  0.545227303896918 PR-AUC:  0.6674916000028444\n",
      "RF\n",
      "accuracy:  0.5882814898021825 precision:  0.5916279151800284 recall:  0.5882814898021825 f1-score:  0.5844876650167337 ROC-AUC:  0.5882814898021825 PR-AUC:  0.7080173396605481\n"
     ]
    }
   ],
   "source": [
    "print(\"LR\")\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "lr.fit(scores_train, labels_train)\n",
    "predictions = lr.predict(scores_test)\n",
    "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n",
    "\n",
    "print(\"NB\")\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(scores_train, labels_train)\n",
    "predictions = gnb.predict(scores_test)\n",
    "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n",
    "\n",
    "print(\"RF\")\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(scores_train, labels_train)\n",
    "predictions = clf.predict(scores_test)\n",
    "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxLv9xd4bFV8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WHBGPmHbGF8"
   },
   "outputs": [],
   "source": [
    "******************************************************************** GAT with ML ******************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EaJeGFxobFln"
   },
   "outputs": [],
   "source": [
    "from dgl.nn import GATConv\n",
    "\n",
    "# ----------- 2. create model -------------- #\n",
    "# build a two-layer GraphSAGE model\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_feats, h_feats, num_heads=1)\n",
    "        self.conv2 = GATConv(h_feats, h_feats,num_heads=1)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "model = GAT(LEN, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7Wg1jBydbl1S"
   },
   "outputs": [],
   "source": [
    "h = model(g_for_baseline, e_feat)\n",
    "h=h.reshape(LEN,128)\n",
    "pos_score_train = decoder(train_pos_g, h)\n",
    "neg_score_train = decoder(train_neg_g, h)\n",
    "pos_score_test = decoder(test_pos_g, h)\n",
    "neg_score_test = decoder(test_neg_g, h)\n",
    "\n",
    "scores_train = torch.cat([pos_score_train, neg_score_train]).detach().numpy()\n",
    "labels_train = torch.cat([torch.ones(pos_score_train.shape[0]), torch.zeros(neg_score_train.shape[0])]).detach().numpy()\n",
    "scores_test = torch.cat([pos_score_test, neg_score_test]).detach().numpy()\n",
    "labels_test = torch.cat([torch.ones(pos_score_test.shape[0]), torch.zeros(neg_score_test.shape[0])]).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 149908,
     "status": "ok",
     "timestamp": 1657215490460,
     "user": {
      "displayName": "kms hamim",
      "userId": "17071813694682210730"
     },
     "user_tz": 300
    },
    "id": "qcxRNSgibf08",
    "outputId": "2108c639-a4c3-446d-dba6-2bd84a19c19e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "accuracy:  0.690282131661442 precision:  0.7055220776873141 recall:  0.690282131661442 f1-score:  0.6844321060518024 ROC-AUC:  0.6902821316614419 PR-AUC:  0.7813874477019046\n",
      "NB\n",
      "accuracy:  0.6459770114942529 precision:  0.7669291589492024 recall:  0.6459770114942529 f1-score:  0.6007494588816885 ROC-AUC:  0.6459770114942529 PR-AUC:  0.7892609409191801\n",
      "RF\n",
      "accuracy:  0.7202455590386625 precision:  0.7472169511055602 recall:  0.7202455590386625 f1-score:  0.7124012978347476 ROC-AUC:  0.7202455590386626 PR-AUC:  0.8041294900541032\n"
     ]
    }
   ],
   "source": [
    "print(\"LR\")\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "lr.fit(scores_train, labels_train)\n",
    "predictions = lr.predict(scores_test)\n",
    "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n",
    "\n",
    "print(\"NB\")\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(scores_train, labels_train)\n",
    "predictions = gnb.predict(scores_test)\n",
    "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n",
    "\n",
    "print(\"RF\")\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(scores_train, labels_train)\n",
    "predictions = clf.predict(scores_test)\n",
    "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbQB0DiBxK9C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXZ_MHWbzRCw"
   },
   "outputs": [],
   "source": [
    "***************************************************************** DeepWalk **********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0jc4LGYt5UI"
   },
   "outputs": [],
   "source": [
    "g_for_baseline_adj_matrix=g_for_baseline.adjacency_matrix_scipy()\n",
    "G_baseline = nx.from_numpy_array(g_for_baseline_adj_matrix,create_using=nx.DiGraph)\n",
    "nx.write_edgelist(G_baseline, \"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/G_baseline_DB.txt\", data=False)\n",
    "\n",
    "# train model and generate embedding\n",
    "model = DeepWalk(walk_length=100, dimensions=128, window_size=5)\n",
    "model.fit(G_baseline)\n",
    "h = model.get_embedding()\n",
    "h=torch.from_numpy(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXqloGE1n7tQ"
   },
   "outputs": [],
   "source": [
    "pos_score_train = decoder(train_pos_g, h)\n",
    "neg_score_train = decoder(train_neg_g, h)\n",
    "pos_score_test = decoder(test_pos_g, h)\n",
    "neg_score_test = decoder(test_neg_g, h)\n",
    "\n",
    "scores_train = torch.cat([pos_score_train, neg_score_train]).detach().numpy()\n",
    "labels_train = torch.cat([torch.ones(pos_score_train.shape[0]), torch.zeros(neg_score_train.shape[0])]).detach().numpy()\n",
    "scores_test = torch.cat([pos_score_test, neg_score_test]).detach().numpy()\n",
    "labels_test = torch.cat([torch.ones(pos_score_test.shape[0]), torch.zeros(neg_score_test.shape[0])]).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127695,
     "status": "ok",
     "timestamp": 1657213005957,
     "user": {
      "displayName": "kms hamim",
      "userId": "17071813694682210730"
     },
     "user_tz": 300
    },
    "id": "w2l0fnbN0oDw",
    "outputId": "44f73a80-d4ea-4c80-8bf7-6df0952d7355"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "accuracy:  0.7335292580982237 precision:  0.7338144849239618 recall:  0.7335292580982237 f1-score:  0.733447967379448 ROC-AUC:  0.7335292580982237 PR-AUC:  0.8005722115536301\n",
      "NB\n",
      "accuracy:  0.7077194357366771 precision:  0.7081833293534113 recall:  0.7077194357366771 f1-score:  0.7075565232420205 ROC-AUC:  0.7077194357366771 PR-AUC:  0.78200849753797\n",
      "RF\n",
      "accuracy:  0.7378657262277952 precision:  0.737966308416069 recall:  0.7378657262277952 f1-score:  0.7378380239603947 ROC-AUC:  0.7378657262277951 PR-AUC:  0.8035732863302465\n"
     ]
    }
   ],
   "source": [
    "print(\"LR\")\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "lr.fit(scores_train, labels_train)\n",
    "predictions = lr.predict(scores_test)\n",
    "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n",
    "\n",
    "print(\"NB\")\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(scores_train, labels_train)\n",
    "predictions = gnb.predict(scores_test)\n",
    "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n",
    "\n",
    "print(\"RF\")\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(scores_train, labels_train)\n",
    "predictions = clf.predict(scores_test)\n",
    "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nwHYtgNexkhO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dStwQAqB4WX3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0ENKLDg4R72"
   },
   "outputs": [],
   "source": [
    "******************************************************************* Node2Vec ***********************************8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8b2TBKC4YWT"
   },
   "outputs": [],
   "source": [
    "h=np.loadtxt('/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/node_2vec_embeddings_DB.txt',delimiter=',')\n",
    "h=torch.from_numpy(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVfHtc63XuZa"
   },
   "outputs": [],
   "source": [
    "pos_score_train = decoder(train_pos_g, h)\n",
    "neg_score_train = decoder(train_neg_g, h)\n",
    "pos_score_test = decoder(test_pos_g, h)\n",
    "neg_score_test = decoder(test_neg_g, h)\n",
    "\n",
    "scores_train = torch.cat([pos_score_train, neg_score_train]).detach().numpy()\n",
    "labels_train = torch.cat([torch.ones(pos_score_train.shape[0]), torch.zeros(neg_score_train.shape[0])]).detach().numpy()\n",
    "scores_test = torch.cat([pos_score_test, neg_score_test]).detach().numpy()\n",
    "labels_test = torch.cat([torch.ones(pos_score_test.shape[0]), torch.zeros(neg_score_test.shape[0])]).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37634,
     "status": "ok",
     "timestamp": 1657258556189,
     "user": {
      "displayName": "kms hamim",
      "userId": "17071813694682210730"
     },
     "user_tz": 300
    },
    "id": "c6xQI7lWXw7M",
    "outputId": "6aec206e-87f6-4757-b325-114eaac729ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "accuracy:  0.7941627862931929 precision:  0.7961634235308683 recall:  0.7941627862931929 f1-score:  0.7938145813887677 ROC-AUC:  0.794162786293193 PR-AUC:  0.8447253247083487\n",
      "NB\n",
      "accuracy:  0.723205830682924 precision:  0.7249722389428003 recall:  0.723205830682924 f1-score:  0.7226614378176941 ROC-AUC:  0.7232058306829239 PR-AUC:  0.7943964278615755\n",
      "RF\n",
      "accuracy:  0.708978520255223 precision:  0.7266592946543649 recall:  0.708978520255223 f1-score:  0.7031902891290899 ROC-AUC:  0.708978520255223 PR-AUC:  0.7938337538991466\n"
     ]
    }
   ],
   "source": [
    "print(\"LR\")\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "lr.fit(scores_train, labels_train)\n",
    "predictions = lr.predict(scores_test)\n",
    "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n",
    "\n",
    "print(\"NB\")\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(scores_train, labels_train)\n",
    "predictions = gnb.predict(scores_test)\n",
    "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n",
    "\n",
    "print(\"RF\")\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(scores_train, labels_train)\n",
    "predictions = clf.predict(scores_test)\n",
    "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szIJQZdhYRnH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
